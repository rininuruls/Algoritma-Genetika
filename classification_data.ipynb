{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rininuruls/Algoritma-Genetika/blob/main/classification_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SkwF7xgV3xSw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, CountVectorizer, IDF,\n",
        "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        ")\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inisialisasi SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BeasiswaBAZNASClassification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load CSV\n",
        "df = spark.read.csv(\"/content/Dataset_Beasiswa_BAZNAS_Indonesia.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Tampilkan data\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIWfalVS33_T",
        "outputId": "794de58c-338e-49b2-ab94-3d3f5d25fc84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------------------+-----------------+----+--------+----+----------------+--------------------+-------------------+-------------+\n",
            "| ID|   Nama|Pendapatan_Keluarga|Jumlah_Tanggungan| IPK|Semester|Usia|Status_Orang_Tua|Aktivitas_Organisasi|Pekerjaan_Sampingan|Skor_Motivasi|\n",
            "+---+-------+-------------------+-----------------+----+--------+----+----------------+--------------------+-------------------+-------------+\n",
            "|  1|   Gita|            3213573|                5|3.94|       4|  19|           Yatim|               Aktif|                Ada|           91|\n",
            "|  2|   Tari|            1525893|                4|3.23|       1|  18|     Yatim Piatu|         Tidak Aktif|                Ada|           91|\n",
            "|  3|   Omar|            4610102|                4|2.66|       7|  24|           Yatim|               Aktif|          Tidak Ada|           69|\n",
            "|  4|Kartini|            3169455|                3|3.32|       7|  22|           Yatim|         Tidak Aktif|                Ada|           58|\n",
            "|  5| Hendra|            3255363|                3|3.18|       1|  22|           Piatu|         Tidak Aktif|                Ada|           66|\n",
            "+---+-------+-------------------+-----------------+----+--------+----+----------------+--------------------+-------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Buat label klasifikasi Beasiswa: 1 = Layak, 0 = Tidak Layak\n",
        "df = df.withColumn(\n",
        "    \"label\",\n",
        "    when((df.IPK >= 3.2) & (df.Pendapatan_Keluarga < 4000000), 1).otherwise(0)\n",
        ")"
      ],
      "metadata": {
        "id": "iUENSxHK41xF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitur_numerik = [\"Pendapatan_Keluarga\", \"Jumlah_Tanggungan\", \"IPK\", \"Semester\", \"Usia\", \"Skor_Motivasi\"]\n"
      ],
      "metadata": {
        "id": "q3Whics05Hao"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing dan One-Hot Encoding\n",
        "status_indexer = StringIndexer(inputCol=\"Status_Orang_Tua\", outputCol=\"status_index\", handleInvalid=\"keep\")\n",
        "status_encoder = OneHotEncoder(inputCol=\"status_index\", outputCol=\"status_encoded\")\n",
        "\n",
        "organisasi_indexer = StringIndexer(inputCol=\"Aktivitas_Organisasi\", outputCol=\"organisasi_index\", handleInvalid=\"keep\")\n",
        "organisasi_encoder = OneHotEncoder(inputCol=\"organisasi_index\", outputCol=\"organisasi_encoded\")\n",
        "\n",
        "pekerjaan_indexer = StringIndexer(inputCol=\"Pekerjaan_Sampingan\", outputCol=\"pekerjaan_index\", handleInvalid=\"keep\")\n",
        "pekerjaan_encoder = OneHotEncoder(inputCol=\"pekerjaan_index\", outputCol=\"pekerjaan_encoded\")"
      ],
      "metadata": {
        "id": "klphUKHn5KpQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "# Gabungkan fitur numerik ke dalam vektor sebelum scaling\n",
        "numerik_assembler = VectorAssembler(\n",
        "    inputCols=[\"Pendapatan_Keluarga\", \"Jumlah_Tanggungan\", \"IPK\", \"Semester\", \"Usia\", \"Skor_Motivasi\"],\n",
        "    outputCol=\"fitur_numerik_vec\"\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "numerik_scaler = StandardScaler(\n",
        "    inputCol=\"fitur_numerik_vec\",\n",
        "    outputCol=\"fitur_numerik_scaled\"\n",
        ")"
      ],
      "metadata": {
        "id": "77rov42y5fKj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_assembler = VectorAssembler(\n",
        "    inputCols=[\"fitur_numerik_scaled\", \"status_encoded\", \"organisasi_encoded\", \"pekerjaan_encoded\"],\n",
        "    outputCol=\"features\"\n",
        ")"
      ],
      "metadata": {
        "id": "I-gEaPrO5kt2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "classifier = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    seed=42,\n",
        "    numTrees=100,\n",
        "    maxDepth=5\n",
        ")"
      ],
      "metadata": {
        "id": "cbnNdoOo5ogr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "\n",
        "# Tokenisasi teks dari kolom Skor_Motivasi\n",
        "tokenizer = Tokenizer(inputCol=\"Skor_Motivasi\", outputCol=\"words\")\n",
        "\n",
        "# Hapus stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "\n",
        "# Vectorisasi kata-kata\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n",
        "\n",
        "# TF-IDF\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"motivation_features\")\n"
      ],
      "metadata": {
        "id": "DtHBDoFf5u6W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# 1. Konversi nilai numerik ke string (sudah dilakukan)\n",
        "df = df.withColumn(\"Skor_Motivasi_Str\", col(\"Skor_Motivasi\").cast(\"string\"))\n",
        "\n",
        "# 2. Text preprocessing\n",
        "tokenizer = Tokenizer(inputCol=\"Skor_Motivasi_Str\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"title_features\")\n",
        "\n",
        "# 3. Categorical encoding (misalnya: Status_Orang_Tua)\n",
        "status_indexer = StringIndexer(inputCol=\"Status_Orang_Tua\", outputCol=\"status_index\", handleInvalid=\"keep\")\n",
        "status_encoder = OneHotEncoder(inputCol=\"status_index\", outputCol=\"status_encoded\")\n",
        "\n",
        "# 4. Combine features\n",
        "feature_assembler = VectorAssembler(\n",
        "    inputCols=[\"title_features\", \"status_encoded\"],  # tambahkan fitur lain jika perlu\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# 5. Random Forest\n",
        "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
        "\n",
        "# 6. Pipeline lengkap\n",
        "pipeline = Pipeline(stages=[\n",
        "    tokenizer,\n",
        "    remover,\n",
        "    vectorizer,\n",
        "    idf,\n",
        "    status_indexer,\n",
        "    status_encoder,\n",
        "    feature_assembler,\n",
        "    classifier\n",
        "])\n"
      ],
      "metadata": {
        "id": "tfwhkTozSNQ-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Ubah Skor_Motivasi menjadi string untuk diproses sebagai teks\n",
        "df = df.withColumn(\"Skor_Motivasi_Str\", col(\"Skor_Motivasi\").cast(\"string\"))"
      ],
      "metadata": {
        "id": "SQjD5ApzS8xj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"Skor_Motivasi_Str\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"title_features\")\n"
      ],
      "metadata": {
        "id": "PhbJ1kLYTFJu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data menjadi train dan test\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "gehKNdcQ5xft"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih model dengan data latih\n",
        "model = pipeline.fit(train_df)\n"
      ],
      "metadata": {
        "id": "W7rMPdoATwqH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi menggunakan data test\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "# Evaluasi akurasi model\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\"\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Akurasi Model: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUF1Z-pc55X2",
        "outputId": "52a576f7-f149-4fbe-97d3-bd9f0b6b6c66"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi Model: 0.5432098765432098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Tambahkan label_name dari label asli\n",
        "predictions = predictions.withColumn(\n",
        "    \"label_name\",\n",
        "    when(predictions.label == 0, \"Low\")\n",
        "    .when(predictions.label == 1, \"Medium\")\n",
        "    .otherwise(\"High\")\n",
        ")\n",
        "\n",
        "# Tambahkan predicted_label_name dari hasil prediksi\n",
        "predictions = predictions.withColumn(\n",
        "    \"predicted_label_name\",\n",
        "    when(predictions.prediction == 0, \"Low\")\n",
        "    .when(predictions.prediction == 1, \"Medium\")\n",
        "    .otherwise(\"High\")\n",
        ")"
      ],
      "metadata": {
        "id": "Pnjw-wMa7tp5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = predictions.select(\"judul\", \"harga\", \"terjual\",\"lokasi\", \"label_name\", \"prediction\")\n",
        "save.write.csv(\"predictions.csv\", header=True)"
      ],
      "metadata": {
        "id": "XRys69B28ofx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}